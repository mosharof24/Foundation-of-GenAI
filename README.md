# Deep Generative Models â€” Theory to Practice

A **math-first, implementation-oriented** collection of Python notebooks and reference code for **Deep Generative Models (DGMs)**.  
This repository bridges **probabilistic foundations**, **optimization theory**, and **modern architectures** powering todayâ€™s image generators and Large Language Models (LLMs).

> _Learning generative models is like learning to cook without a recipe â€” divergences are taste tests, GANs are chef vs critic, diffusion sculpts order from noise, and alignment tunes the dish to human preference._

---

## ğŸ“Œ Contents Overview

This repo covers **both theory and code**, progressing from first principles to state-of-the-art models.

---

## ğŸ§  Topics Covered

### 1ï¸âƒ£ Mathematical & Probabilistic Foundations
- Probability distributions and density modeling  
- Latent variable models  
- Jensenâ€™s Inequality and variational bounds  
- Law of Large Numbers  
- Manifold Hypothesis  
- Score-based modeling and Tweedieâ€™s formula  

---

### 2ï¸âƒ£ Divergence Measures & Optimal Transport
- $f$-divergence family  
  - KL-divergence  
  - Jensenâ€“Shannon divergence  
  - Total Variation distance  
- Variational Divergence Minimization (VDM)  
- Optimal Transport theory  
- Wasserstein distance and geometry of distributions  
- Wasserstein GAN (WGAN) motivation and formulation  

---

### 3ï¸âƒ£ Generative Adversarial Networks (GANs)
- Adversarial learning as saddle-point optimization  
- Generatorâ€“Discriminator dynamics  
- DCGAN architecture (convolutional GANs)  
- Conditional GANs  
- Training instabilities and mode collapse  
- GANs under the Manifold Hypothesis  

---

### 4ï¸âƒ£ Variational Autoencoders (VAEs)
- Latent variable formulation  
- Evidence Lower Bound (ELBO) derivation  
- Reparameterization trick  
- Posterior collapse  
- Vector Quantized VAE (VQ-VAE)  
- Discrete vs continuous latent spaces  

---

### 5ï¸âƒ£ Diffusion Models
- Forward noising process  
- Reverse denoising process  
- Denoising Diffusion Probabilistic Models (DDPM)  
- Denoising Diffusion Implicit Models (DDIM)  
- Score matching interpretation  
- Latent Diffusion Models (LDM)  

---

### 6ï¸âƒ£ Auto-Regressive Models
- Autoregressive factorization of likelihood  
- Recurrent Neural Networks (RNNs)  
- Transformers  
- Self-attention mechanism  
- Multi-Head Attention  
- Causal masking and decoding  

---

### 7ï¸âƒ£ State-Space Models (SSMs)
- Continuous and discrete state-space models  
- Linear SSM formulation  
- S4 (Structured State Space Sequence) models  
- Selective State-Space Models (Mamba)  
- Parallel scan algorithms vs convolutional kernels  

---

### 8ï¸âƒ£ Model Evaluation & Practical Techniques
- Frechet Inception Distance (FID)  
- GAN inversion and latent editing  
- Unsupervised Domain Adaptation (UDA)  
- Teacher forcing for sequence models  

---

### 9ï¸âƒ£ Model Alignment & Reinforcement Learning
- Alignment problem in LLMs  
- Reinforcement Learning from Human Feedback (RLHF)  
- Reward modeling  
- Bradleyâ€“Terry preference model  
- Proximal Policy Optimization (PPO)  
- Trust Region Policy Optimization (TRPO)  
- Direct Preference Optimization (DPO)  

---

### ğŸ”Ÿ Applications & Case Studies
- Large Language Models (ChatGPT, Claude, Gemini)  
- Image generation (DALLÂ·E, Stable Diffusion)  
- Multimodal generative systems  

---

## ğŸ› ï¸ Implementation Details
- Python + PyTorch based notebooks  
- Minimal, readable implementations  
- Emphasis on clarity over abstraction  
- Mathematical derivations paired with code  
- Reproducible experiments and visualizations  

---

## ğŸ“‚ Repository Structure (Planned)
```text
.
â”œâ”€â”€ foundations/
â”œâ”€â”€ divergences/
â”œâ”€â”€ gans/
â”œâ”€â”€ vae/
â”œâ”€â”€ diffusion/
â”œâ”€â”€ autoregressive/
â”œâ”€â”€ ssm/
â”œâ”€â”€ alignment/
â”œâ”€â”€ evaluation/
â”œâ”€â”€ notebooks/
â””â”€â”€ utils/
